# Emotion Recognition from Speech Model 
# Overview
This repository contains the Emotion Recognition from Speech project, which aims to identify and classify emotions expressed in spoken language. By utilizing machine learning and deep learning techniques, the project analyzes audio features to detect emotions such as happiness, sadness, anger, and more.

# Usage
To use the credit scoring model, follow these steps:

Ensure you have Python installed on your machine.
Open the Jupyter notebook credit_score.ipynb.
Follow the instructions provided in the notebook to load the dataset, preprocess the data, train the classification models, and assess their accuracies.
Experiment with different algorithms and parameters to optimize the model performance.

# Dataset
The dataset includes audio recordings labeled with different emotions. Commonly used datasets include:
RAVDESS (The Ryerson Audio-Visual Database of Emotional Speech and Song)
EMO-DB (Berlin Database of Emotional Speech)

# Project Goals
Emotion Detection: Develop a model to accurately detect emotions from speech.
Feature Extraction: Identify key audio features that contribute to emotion recognition.
Real-time Application: Explore the potential for real-time emotion recognition in applications such as virtual assistants and customer service.

# Technologies Used
Python
Librosa (for audio processing)
TensorFlow/Keras (for deep learning)
Scikit-learn (for machine learning)
Matplotlib/Seaborn (for visualization)

# Results
The accuracies of different classification models are as follows:

Logistic Regression: 69.6%
Support Vector Machine (SVM): 93.5%
K-Nearest Neighbors (KNN): 89.7%
Random Forest: 96%

# License
This project is licensed under the MIT License.
